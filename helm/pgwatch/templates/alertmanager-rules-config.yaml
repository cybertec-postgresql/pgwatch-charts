{{- if eq .Values.pgwatch.prometheus.new_prometheus.create_alertmanager "true" }}
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    application: pgwatch
  name: alertmanager-rules-config
data:
  pgwatch-monitoring-default-alert-rules.yml: |
    groups:
    - name: alert-rules
      rules:

    #Exporter-Alerts
      - alert: MetricExporterReportsErrors
        expr: pg_exporter_last_scrape_error > 0
        for: 60s
        labels:
          service: postgresql
          severity: medium
          severity_num: 300
        annotations:
          summary: '{{ $labels.clusterid }}: The exporter reports errors when retrieving data from Pod: {{ $labels.clustername }}. (Number of errors: ( {{ $value }} )'


    #PostgreSQL-Alerts
      - alert: PGPodDown
        expr: pg_up < 1
        for: 60s
        labels:
          service: postgresql
          severity: critical
          severity_num: 400
        annotations:
          summary: '{{ $labels.clusterid }}: Pod {{ $labels.clustername }} seems to be down. The exporter is not able to communicate with the database on Pod.'

    
    #  - alert: PGbelowMinMajorVersion
    #    expr:  pg_settings_server_version_num < 140000
    #    for: 300s
    #    labels:
    #      service: postgresql
    #      severity: info
    #      severity_num: 100
    #    annotations:
    #      summary: '{{ $labels.clusterid }}: Cluster uses an older major version than defined.'

    #  - alert: PGbelowMinVersion
    #    expr:  pg_settings_server_version_num < 1200013
    #    for: 300s
    #    labels:
    #      service: postgresql
    #      severity: medium
    #      severity_num: 300
    #    annotations:
    #      summary: '{{ $labels.clusterid }}: Cluster uses an older version than defined.'


    # Whether a system switches from primary to replica or vice versa must be configured per named job.
    # No way to tell what value a system is supposed to be without a rule expression for that specific system
    # 2 to 1 means it changed from primary to replica. 1 to 2 means it changed from replica to primary
    # Set this alert for each system that you want to monitor a recovery status change
    # Below is an example for a target job called "Replica" and watches for the value to change above 1 which means it's no longer a replica
    #
    #  - alert: PGRecoveryStatusSwitch_Replica
    #    expr: ccp_is_in_recovery_status{job="Replica"} > 1
    #    for: 60s
    #    labels:
    #      service: postgresql
    #      severity: critical
    #      severity_num: 400
    #    annotations:
    #      summary: '{{ $labels.job }} has changed from replica to primary'


    # Absence alerts must be configured per named job, otherwise there's no way to know which job is down
    # Below is an example for a target job called "Prod"
    #  - alert: PGConnectionAbsent_Prod
    #    expr: absent(ccp_connection_stats_max_connections{job="Prod"})
    #    for: 10s
    #    labels:
    #      service: postgresql
    #      severity: critical
    #      severity_num: 400
    #    annotations:
    #      description: 'Connection metric is absent from target (Prod). Check that postgres_exporter can connect to PostgreSQL.'


    # Optional monitor for changes to pg_settings (postgresql.conf) system catalog.
    # A similar metric is available for monitoring pg_hba.conf. See ccp_hba_settings_checksum().
    # If metric returns 0, then NO settings have changed for either pg_settings since last known valid state
    # If metric returns 1, then pg_settings have changed since last known valid state
    # To see what may have changed, check the monitor.pg_settings_checksum table for a history of config state.
    #  - alert: PGSettingsChecksum
    #    expr: ccp_pg_settings_checksum > 0
    #    for 60s
    #    labels:
    #      service: postgresql
    #      severity: critical
    #      severity_num: 400
    #    annotations:
    #      description: 'Configuration settings on {{ $labels.job }} have changed from previously known valid state. To reset current config to a valid state after alert fires, run monitor.pg_settings_checksum_set_valid().'
    #      summary: 'PGSQL Instance settings checksum'


    # Monitor for data block checksum failures. Only works in PG12+
    #  - alert: PGDataChecksum
    #    expr: ccp_data_checksum_failure > 0
    #    for 60s
    #    labels:
    #      service: postgresql
    #      severity: critical
    #      severity_num: 400
    #    annotations:
    #      description: '{{ $labels.job }} has at least one data checksum failure in database {{ $labels.dbname }}. See pg_stat_database system catalog for more information.'
    #      summary: 'PGSQL Data Checksum failure'

      # - alert: PGConnIdleTxn_warning
      #   expr: pgwatch_stat_connections_max_idle_in_transaction_time > 300
      #   for: 60s
      #   labels:
      #     service: postgresql
      #     severity: medium
      #     severity_num: 300
      #   annotations:
      #     description: '{{ $labels.clusterid }}: There is at least one connection which stays as "idle in transaction" for over 5 minutes.'
      #     summary: 'PGSQL Instance idle transactions'

      # - alert: PGConnIdleTxn_critical
      #   expr: pgwatch_stat_connections_max_idle_in_transaction_time > 900
      #   for: 60s
      #   labels:
      #     service: postgresql
      #     severity: critical
      #     severity_num: 400
      #   annotations:
      #     description: '{{ $labels.clusterid }}: There is at least one connection which stays as "idle in transaction" for over 15 minutes.'
      #     summary: 'PGSQL Instance idle transactions'

      # - alert: PGQueryTime
      #   expr: ccp_connection_stats_max_query_time > 43200
      #   for: 60s
      #   labels:
      #     service: postgresql
      #     severity: warning
      #     severity_num: 200
      #   annotations:
      #     description: '{{ $labels.job }} has at least one query running for over 12 hours.'
      #     summary: 'PGSQL Max Query Runtime'


      # - alert: PGQueryTime
      #   expr: ccp_connection_stats_max_query_time > 86400
      #   for: 60s
      #   labels:
      #     service: postgresql
      #     severity: critical
      #     severity_num: 400
      #   annotations:
      #     description: '{{ $labels.job }} has at least one query running for over 1 day.'
      #     summary: 'PGSQL Max Query Runtime'

      # - alert: PGConnUsage_Low
      #   expr: (pgwatch_stat_connections_total_connections / pg_settings_max_connections) > 0.8
      #   for: 180s
      #   labels:
      #     service: postgresql
      #     severity: low
      #     severity_num: 200
      #   annotations:
      #     description: '{{ $labels.clusterid }}: 80% or more of the available connections are in use ({{ $value }}%)'
      #     summary: 'PGSQL Instance connections'

      # - alert: PGConnUsage_Warning
      #   expr: (pgwatch_stat_connections_total_connections / pg_settings_max_connections) > 0.85
      #   for: 120s
      #   labels:
      #     service: postgresql
      #     severity: medium
      #     severity_num: 300
      #   annotations:
      #     description: '{{ $labels.clusterid }}: 85% or more of the available connections are in use ({{ $value }}%)'
      #     summary: 'PGSQL Instance connections'

      # - alert: PGConnUsage_Critical
      #   expr: (ccp_connection_stats_total / ccp_connection_stats_max_connections) > 0.9
      #   for: 120s
      #   labels:
      #     service: postgresql
      #     severity: critical
      #     severity_num: 400
      #   annotations:
      #     description: '{{ $labels.clusterid }}: 90% or more of the available connections are in use ({{ $value }}%)'
      #     summary: 'PGSQL Instance connections'

      # - alert: PGDiskUsage_Low
      #   expr: ((pgwatch_pgnodemx_data_disk_total_bytes - pgwatch_pgnodemx_data_disk_available_bytes) / pgwatch_pgnodemx_data_disk_total_bytes) > 0.7
      #   for: 300s
      #   labels:
      #     service: postgresql
      #     severity: low
      #     severity_num: 200
      #   annotations:
      #     description: '{{ $labels.clusterid }}: The storage usage of the pod {{ $labels.clustername }} has exceeded the limit of 75% on Volume: {{ $labels.mount_point }} ({{ $value }}).'
      #     summary: PG-Cluster Storage-Usage - Low
        
      # - alert: PGDiskUsage_Warning
      #   expr: ((pgwatch_pgnodemx_data_disk_total_bytes - pgwatch_pgnodemx_data_disk_available_bytes) / pgwatch_pgnodemx_data_disk_total_bytes) > 0.8
      #   for: 180s
      #   labels:
      #     service: postgresql
      #     severity: medium
      #     severity_num: 300
      #   annotations:
      #     description: '{{ $labels.clusterid }}: The storage usage of the pod {{ $labels.clustername }} has exceeded the limit of 80% on Volume: {{ $labels.mount_point }} ({{ $value }}).'
      #     summary: PG-Cluster Storage-Usage - Medium

      # - alert: PGDiskUsage_Critical
      #   expr: ((pgwatch_pgnodemx_data_disk_total_bytes - pgwatch_pgnodemx_data_disk_available_bytes) / pgwatch_pgnodemx_data_disk_total_bytes) > 0.9
      #   for: 120s
      #   labels:
      #     service: postgresql
      #     severity: critical
      #     severity_num: 400
      #   annotations:
      #     description: '{{ $labels.clusterid }}: The storage usage of the pod {{ $labels.clustername }} has exceeded the limit of 90% on Volume: {{ $labels.mount_point }} ({{ $value }}).'
      #     summary: PG-Cluster Storage-Usage - Critical

      # - alert: PGReplicationByteLag
      #   expr: ccp_replication_status_byte_lag > 5.24288e+07
      #   for: 60s
      #   labels:
      #     service: postgresql
      #     severity: warning
      #     severity_num: 200
      #   annotations:
      #     description: 'PGSQL Instance {{ $labels.job }} has at least one replica lagging over 50MB behind.'
      #     summary: 'PGSQL Instance replica lag warning'

      # - alert: PGReplicationLaginByzes
      #   expr: pg_stat_replication_pg_wal_lsn_diff > 1.048576e+08
      #   for: 60s
      #   labels:
      #     service: postgresql
      #     severity: critical
      #     severity_num: 400
      #   annotations:
      #     description: 'PGSQL Instance {{ $labels.job }} has at least one replica lagging over 100MB behind.'
      #     summary: 'PGSQL Instance replica lag warning'

      # - alert: PGReplicationSlotsInactive
      #   expr: ccp_replication_slots_active == 0
      #   for: 60s
      #   labels:
      #     service: postgresql
      #     severity: critical
      #     severity_num: 400
      #   annotations:
      #     description: 'PGSQL Instance {{ $labels.job }} has one or more inactive replication slots'
      #     summary: 'PGSQL Instance inactive replication slot'

      # - alert: PGXIDWraparound
      #   expr: ccp_transaction_wraparound_percent_towards_wraparound > 50
      #   for: 60s
      #   labels:
      #     service: postgresql
      #     severity: warning
      #     severity_num: 200
      #   annotations:
      #     description: 'PGSQL Instance {{ $labels.job }} is over 50% towards transaction id wraparound.'
      #     summary: 'PGSQL Instance {{ $labels.job }} transaction id wraparound imminent'

      # - alert: PGXIDWraparound
      #   expr: ccp_transaction_wraparound_percent_towards_wraparound > 75
      #   for: 60s
      #   labels:
      #     service: postgresql
      #     severity: critical
      #     severity_num: 400
      #   annotations:
      #     description: 'PGSQL Instance {{ $labels.job }} is over 75% towards transaction id wraparound.'
      #     summary: 'PGSQL Instance transaction id wraparound imminent'

      # - alert: PGEmergencyVacuum
      #   expr: ccp_transaction_wraparound_percent_towards_emergency_autovac > 110
      #   for: 60s
      #   labels:
      #     service: postgresql
      #     severity: warning
      #     severity_num: 200
      #   annotations:
      #     description: 'PGSQL Instance {{ $labels.job }} is over 110% beyond autovacuum_freeze_max_age value. Autovacuum may need tuning to better keep up.'
      #     summary: 'PGSQL Instance emergency vacuum imminent'

      # - alert: PGEmergencyVacuum
      #   expr: ccp_transaction_wraparound_percent_towards_emergency_autovac > 125
      #   for: 60s
      #   labels:
      #     service: postgresql
      #     severity: critical
      #     severity_num: 400
      #   annotations:
      #     description: 'PGSQL Instance {{ $labels.job }} is over 125% beyond autovacuum_freeze_max_age value. Autovacuum needs tuning to better keep up.'
      #     summary: 'PGSQL Instance emergency vacuum imminent'

      # - alert: PGArchiveCommandStatus
      #   expr: ccp_archive_command_status_seconds_since_last_fail > 300
      #   for: 60s
      #   labels:
      #       service: postgresql
      #       severity: critical
      #       severity_num: 400
      #   annotations:
      #       description: 'PGSQL Instance {{ $labels.job }} has a recent failing archive command'
      #       summary: 'Seconds since the last recorded failure of the archive_command'

      # - alert: PGSequenceExhaustion
      #   expr: ccp_sequence_exhaustion_count > 0
      #   for: 60s
      #   labels:
      #       service: postgresql
      #       severity: critical
      #       severity_num: 400
      #   annotations:
      #       description: 'Count of sequences on instance {{ $labels.job }} at over 75% usage: {{ $value }}. Run following query to see full sequence status: SELECT * FROM monitor.sequence_status() WHERE percent >= 75'

      # - alert: PGSettingsPendingRestart
      #   expr: ccp_settings_pending_restart_count > 0
      #   for: 60s
      #   labels:
      #       service: postgresql
      #       severity: critical
      #       severity_num: 400
      #   annotations:
      #       description: 'One or more settings in the pg_settings system catalog on system {{ $labels.job }} are in a pending_restart state. Check the system catalog for which settings are pending and review postgresql.conf for changes.'

    ########## PGBACKREST RULES ##########
    #
    # Uncomment and customize one or more of these rules to monitor your pgbackrest backups.
    # Full backups are considered the equivalent of both differentials and incrementals since both are based on the last full
    #   And differentials are considered incrementals since incrementals will be based off the last diff if one exists
    #   This avoid false alerts, for example when you don't run diff/incr backups on the days that you run a full
    # Stanza should also be set if different intervals are expected for each stanza.
    #   Otherwise rule will be applied to all stanzas returned on target system if not set.
    #
    # Relevant metric names are:
    #   ccp_backrest_last_full_time_since_completion_seconds
    #   ccp_backrest_last_incr_time_since_completion_seconds
    #   ccp_backrest_last_diff_time_since_completion_seconds
    #
    #  - alert: PGBackRestLastCompletedFull_main
    #    expr: ccp_backrest_last_full_backup_time_since_completion_seconds{stanza="main"} > 604800
    #    for: 60s
    #    labels:
    #       service: postgresql
    #       severity: critical
    #       severity_num: 400
    #    annotations:
    #       summary: 'Full backup for stanza [main] on system {{ $labels.job }} has not completed in the last week.'
    #
    #  - alert: PGBackRestLastCompletedIncr_main
    #    expr: ccp_backrest_last_incr_backup_time_since_completion_seconds{stanza="main"} > 86400
    #    for: 60s
    #    labels:
    #       service: postgresql
    #       severity: critical
    #       severity_num: 400
    #    annotations:
    #       summary: 'Incremental backup for stanza [main] on system {{ $labels.job }} has not completed in the last 24 hours.'
    #
    #
    # Runtime monitoring is handled with a single metric:
    #
    #   ccp_backrest_last_runtime_backup_runtime_seconds
    #
    # Runtime monitoring should have the "backup_type" label set.
    #   Otherwise the rule will apply to the last run of all backup types returned (full, diff, incr)
    # Stanza should also be set if runtimes per stanza have different expected times
    #
    #  - alert: PGBackRestLastRuntimeFull_main
    #    expr: ccp_backrest_last_runtime_backup_runtime_seconds{backup_type="full", stanza="main"} > 14400
    #    for: 60s
    #    labels:
    #       service: postgresql
    #       severity: critical
    #       severity_num: 400
    #    annotations:
    #       summary: 'Expected runtime of full backup for stanza [main] has exceeded 4 hours'
    #
    #  - alert: PGBackRestLastRuntimeDiff_main
    #    expr: ccp_backrest_last_runtime_backup_runtime_seconds{backup_type="diff", stanza="main"} > 3600
    #    for: 60s
    #    labels:
    #       service: postgresql
    #       severity: critical
    #       severity_num: 400
    #    annotations:
    #       summary: 'Expected runtime of diff backup for stanza [main] has exceeded 1 hour'
    ##
    #
    ## If the pgbackrest command fails to run, the metric disappears from the exporter output and the alert never fires.
    ## An absence alert must be configured explicitly for each target (job) that backups are being monitored.
    ## Checking for absence of just the full backup type should be sufficient (no need for diff/incr).
    ## Note that while the backrest check command failing will likely also cause a scrape error alert, the addition of this
    ## check gives a clearer answer as to what is causing it and that something is wrong with the backups.
    #
    #  - alert: PGBackrestAbsentFull_Prod
    #    expr: absent(ccp_backrest_last_full_backup_time_since_completion_seconds{job="Prod"})
    #    for: 10s
    #    labels:
    #      service: postgresql
    #      severity: critical
    #      severity_num: 400
    #    annotations:
    #      description: 'Backup Full status missing for Prod. Check that pgbackrest info command is working on target system.'
{{ end }}